# necessary libraries
library(twitteR)
library(tm)
library(e1071)
library(gmodels)

# in the end I may also access the Twitter API for real time tweets to check if my classifer works
 
 tweets<-read.csv("socialmedia-disaster-tweets-DFE", header = TRUE, sep = ",")
# checking length of tweets and the first few lines stored in tweets
 n <- length(tweets)
head(tweets)
str(tweets)

# converting tweets into dataframe format
df <- do.call("rbind", lapply(tweets, as.data.frame))

#check how many tweets are considered "relevant " and "not relevant"
tweets$choose_one <- factor(tweets$choose_one)

# PROCESSING TEXT DATA FOR ANALYSIS

#create a corpus
myCorpus <- Corpus(VectorSource(df$text))

#Preprocessing the tweets
myCorpus <- tm_map(myCorpus, content_transformer(tolower))

# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)

# strip leading and trailing whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)

# remove stopwords
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

#Stemming
dictCorpus <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)

# inspect the first six "documents"
# how to check what has been changed in the tweets from preprocessing
inspect(myCorpus[1:6])


 # stem completion
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)

# Building a document-term matrix
myDtm <- DocumentTermMatrix(myCorpus, control = list(minWordLength = 1))

# can inspect the document term matrix using the line below
inspect(myDtm[266:270,31:40])

#CREATING TRAINING AND TEST DATASETS

# creating the training and test set, randomized
t_train <- sample(nrow(tweets),floor(nrow(tweets)*0.7))
tweet_raw_train <- tweets[t_train,]
tweet_raw_test <- tweets[-t_train,]

# document-term matrix
tweet_dtm_train <- myDtm[t_train,]
tweet_dtm_test <- myDtm[-t_train,]

tweet_corpus_train <- myCorpus[t_train]
tweet_corpus_test <- myCorpus[-t_train]

# check if the subsets are representative of the twitter data
prop.table(table(tweet_raw_train$choose_one))
prop.table(table(tweet_raw_test$choose_one))

# CREATING INDICATOR FEATURES FOR FREQUENT WORDS

#all words included appear in at least 10 messages
sms_dict <- findFreqTerms(sms_dtm_train, 10)
tweet_train <- DocumentTermMatrix(tweet_corpus_train, list(dictionary = sms_dict))
tweet_test <- DocumentTermMatrix(tweet_corpus_test, list(dictionary = sms_dict))

#converting labels to see if word is included in tweet
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
return(x)
}

# converts each of the columns so that is specifies whether a word is present in the tweet or not
tweet_train <- apply(tweet_train, MARGIN = 2, convert_counts)
tweet_test <- apply(tweet_test, MARGIN = 2, convert_counts)

#TRAINING A MODEL ON THE DATA (applying Naives Bayes)

tweet_classifier <- naiveBayes(tweet_train, tweet_raw_train$choose_one)

#EVALUATING MODEL PERFORMANCE

#making predictions
tweet_test_pred <- predict(tweet_classifier, tweet_test)

#confusion matrix
CrossTable(tweet_test_pred, tweet_raw_test$choose_one, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))



